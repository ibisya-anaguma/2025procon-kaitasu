name: daily-scrape

# デフォルト: 毎日 02:00 JST を想定 (JST = UTC+9 -> UTC 17:00 前日)
on:
  schedule:
    - cron: "0 17 * * *"
  workflow_dispatch: {}

concurrency:
  group: daily-scrape
  cancel-in-progress: true

jobs:
  scrape:
    runs-on: ubuntu-latest
    timeout-minutes: 60
    steps:
      - name: Checkout repo
        uses: actions/checkout@v4

      - name: Set up Python 3.11
        uses: actions/setup-python@v4
        with:
          python-version: "3.11"

      - name: Install apt deps & Google Chrome
        run: |
          sudo apt-get update -y
          sudo apt-get install -y wget unzip gnupg2 ca-certificates
          wget -q -O - https://dl.google.com/linux/linux_signing_key.pub | sudo apt-key add - || true
          echo "deb [arch=amd64] http://dl.google.com/linux/chrome/deb/ stable main" | sudo tee /etc/apt/sources.list.d/google-chrome.list
          sudo apt-get update -y
          sudo apt-get install -y google-chrome-stable
          google-chrome --version

      - name: Cache pip
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('src/jobs/scraper/requirements.txt') }}
          restore-keys: ${{ runner.os }}-pip-

      - name: Install Python deps (scraper)
        run: |
          python -m pip install --upgrade pip
          pip install -r src/jobs/scraper/requirements.txt

      - name: Run scraper (headless)
        run: |
          cd src/jobs/scraper
          OUT="all_products-${GITHUB_RUN_NUMBER}-$(date -u +%Y%m%dT%H%M%SZ).json"
          python scraper.py --headless --output="$OUT"
          ls -lah "$OUT" || true

      - name: Upload results as artifact
        uses: actions/upload-artifact@v4
        with:
          name: scrape-results
          path: src/jobs/scraper/all_products-*.json