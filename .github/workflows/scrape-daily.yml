name: daily-scrape

# 毎日 02:00 JST -> UTC 前日 18:00
on:
  schedule:
    - cron: "0 18 * * *"
  workflow_dispatch: {}

permissions:
  contents: write

concurrency:
  group: daily-scrape-${{ github.ref }}
  cancel-in-progress: false

jobs:
  scrape:
    runs-on: ubuntu-latest
    timeout-minutes: 360

    env:
      SRC_PATH: src/jobs/scraper/all_products.json
      LOG_DIR: src/jobs/scraper/logs
      TARGET_BRANCH: data-latest
      DEST_PATH: data/all_products.json

    steps:
      - name: Checkout repo
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Set up Python 3.11
        uses: actions/setup-python@v4
        with:
          python-version: "3.11"

      - name: Install apt deps & Google Chrome
        run: |
          set -euo pipefail
          sudo apt-get update -y
          sudo apt-get install -y wget gnupg ca-certificates unzip
          wget -q -O - https://dl-ssl.google.com/linux/linux_signing_key.pub | sudo apt-key add -
          echo "deb [arch=amd64] http://dl.google.com/linux/chrome/deb/ stable main" | sudo tee /etc/apt/sources.list.d/google-chrome.list
          sudo apt-get update -y
          sudo apt-get install -y google-chrome-stable || true
          sudo apt-get install -y xvfb

      - name: Cache pip
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('src/jobs/scraper/requirements.txt') }}

      - name: Install Python deps (scraper)
        run: |
          set -euo pipefail
          if [ -f src/jobs/scraper/requirements.txt ]; then
            python -m pip install --upgrade pip
            python -m pip install -r src/jobs/scraper/requirements.txt
          fi

      - name: Ensure log dir
        run: mkdir -p "${{ env.LOG_DIR }}"

      - name: Run scraper (headless) and save logs
        run: |
          set -euo pipefail
          TIMESTAMP=$(date -u +%Y%m%dT%H%M%SZ)
          LOG="${{ env.LOG_DIR }}/scrape-${TIMESTAMP}.log"
          echo "Starting scraper, logging to ${LOG}"
          python -u src/jobs/scraper/scraper.py --headless 2>&1 | tee "$LOG"
          EXIT=${PIPESTATUS[0]}
          echo "Scraper exited with $EXIT"
          if [ "$EXIT" -ne 0 ]; then
            exit "$EXIT"
          fi

      - name: Debug scrape output
        if: always()
        run: |
          set -euo pipefail
          echo "Listing src/jobs/scraper:"
          ls -la src/jobs/scraper || true
          echo "all_products.json exists?:"
          if [ -f "${{ env.SRC_PATH }}" ]; then
            echo "FOUND: ${SRC_PATH}"
            echo "size:"
            wc -c "${{ env.SRC_PATH }}" || true
          else
            echo "MISSING: ${SRC_PATH}"
          fi

      - name: Upload scrape artifact (all_products & logs)
        uses: actions/upload-artifact@v4
        with:
          name: scrape-results
          path: |
            ${{ env.SRC_PATH }}
            ${{ env.LOG_DIR }}/*.log

      - name: Write FIREBASE_SERVICE_ACCOUNT secret to file (if provided)
        shell: bash
        run: |
          set -euo pipefail
          if [ -z "${{ secrets.FIREBASE_SERVICE_ACCOUNT }}" ]; then
            echo "FIREBASE_SERVICE_ACCOUNT not set - skipping writing file"
            exit 0
          fi
          mkdir -p .firebase
          printf '%s' "${{ secrets.FIREBASE_SERVICE_ACCOUNT }}" > .firebase/firebase-key.json
          chmod 600 .firebase/firebase-key.json
          echo "Wrote .firebase/firebase-key.json (permission 600)"

      - name: Commit all_products.json to data-latest branch
        env:
          GIT_AUTHOR_NAME: "github-actions[bot]"
          GIT_AUTHOR_EMAIL: "actions@github.com"
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          SRC_PATH: ${{ env.SRC_PATH }}
          TARGET_BRANCH: ${{ env.TARGET_BRANCH }}
          DEST_PATH: ${{ env.DEST_PATH }}
          REPO: ${{ github.repository }}
        shell: bash
        run: |
          set -euo pipefail
          echo "Preparing to push ${SRC_PATH} -> ${TARGET_BRANCH}:${DEST_PATH}"

          if [ ! -f "$SRC_PATH" ]; then
            echo "No scrape output found at $SRC_PATH - skipping push"
            exit 0
          fi

          # --- CRITICAL: copy to temp BEFORE branch switching (prevents lost file due to checkout) ---
          TMP_SRC=$(mktemp -t all_products.XXXX.json)
          cp "$SRC_PATH" "$TMP_SRC"
          echo "Copied src to temp: $TMP_SRC"

          git config user.name "$GIT_AUTHOR_NAME"
          git config user.email "$GIT_AUTHOR_EMAIL"

          git fetch origin "$TARGET_BRANCH" || true

          if git ls-remote --exit-code --heads origin "$TARGET_BRANCH" >/dev/null 2>&1; then
            echo "Switching to existing branch $TARGET_BRANCH"
            git checkout "$TARGET_BRANCH"
            git reset --hard "origin/$TARGET_BRANCH" || true
          else
            echo "Creating orphan branch $TARGET_BRANCH"
            git checkout --orphan "$TARGET_BRANCH"
            git rm -rf .
            git clean -fdx || true
            git commit --allow-empty -m "init $TARGET_BRANCH"
            git push "https://x-access-token:${GITHUB_TOKEN}@github.com/${REPO}" HEAD:"$TARGET_BRANCH"
          fi

          mkdir -p "$(dirname "$DEST_PATH")"
          cp "$TMP_SRC" "$DEST_PATH"
          git add "$DEST_PATH"

          if git diff --cached --quiet; then
            echo "No changes to $DEST_PATH; nothing to commit"
          else
            git commit -m "ci: update all_products.json (run $GITHUB_RUN_ID)"
            git push "https://x-access-token:${GITHUB_TOKEN}@github.com/${REPO}" HEAD:"$TARGET_BRANCH" --force
            echo "Pushed updated $DEST_PATH to $TARGET_BRANCH"
          fi

          rm -f "$TMP_SRC"

      - name: Post-run cleanup (always)
        if: always()
        run: echo "scrape job finished"